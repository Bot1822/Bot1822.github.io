<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2021/12/31/ORB%E5%8C%B9%E9%85%8D/"/>
      <url>/2021/12/31/ORB%E5%8C%B9%E9%85%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="ORB特征"><a href="#ORB特征" class="headerlink" title="ORB特征"></a>ORB特征</h1><p>包含以下两部分：</p><ol><li>FAST角点提取：与原版不同的是计算了主方向</li><li>BRIEF描述子：使用了先前计算的方向信息</li></ol><h2 id="FAST关键点"><a href="#FAST关键点" class="headerlink" title="FAST关键点"></a>FAST关键点</h2><p>检测思想：如果一个像素与邻域像素差别较大，则可能为角点</p><p>检测过程如下：</p><h2 id="BRIEF描述子"><a href="#BRIEF描述子" class="headerlink" title="BRIEF描述子"></a>BRIEF描述子</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>  BRIEF提供了一种计算二值串的捷径，而并不需要去计算一个类似于SIFT的特征描述子。它需要先平滑图像，然后在特征点周围选择一个Patch，在这个Patch内通过一种选定的方法来挑选出来nd个点对。然后对于每一个点对 (p,q)，我们来比较这两个点的亮度值，如果*I(p)&gt;I(q)*则这个点对生成了二值串中一个的值为1，如果 *I(p)&lt;I(q)*，则对应在二值串中的值为-1，否则为0。所有nd个点对，都进行比较之间，我们就生成了一个nd长的二进制串。</p><p>  对于nd的选择，我们可以设置为128，256或512，这三种参数在OpenCV中都有提供，但是OpenCV中默认的参数是256，这种情况下，非匹配点的汉明距离呈现均值为128比特征的高斯分布。一旦维数选定了，我们就可以用汉明距离来匹配这些描述子了。</p><h3 id="点对选择（各种Pattern）"><a href="#点对选择（各种Pattern）" class="headerlink" title="点对选择（各种Pattern）"></a>点对选择（各种Pattern）</h3><p>设我们在特征点的邻域块大小为S×S内选择nd个点对(p,q)，Calonder的实验中测试了5种采样方法：</p><p>1）在图像块内平均采样；</p><p>2）p和q都符合$\left ( 0,\frac{1}{25}S^2  \right )$的高斯分布；</p><p>3）p符合$\left ( 0,\frac{1}{25}S^2  \right )$的高斯分布，而q符合$\left ( 0,\frac{1}{100}S^2  \right )$的高斯分布；</p><p>4）在空间量化极坐标下的离散位置随机采样</p><p>5）把p固定为$(0,0)$，q在周围平均采样</p><p>下面是上面5种采样方法的结果示意图。</p><p><img src=".%5Ctemp%5C071515564565603.png" class="lazyload placeholder" data-srcset=".%5Ctemp%5C071515564565603.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image"></p><p><img src=".%5Ctemp%5C071516017849941.png" class="lazyload placeholder" data-srcset=".%5Ctemp%5C071516017849941.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image"></p><h1 id="匹配（基于cv的Matcher）"><a href="#匹配（基于cv的Matcher）" class="headerlink" title="匹配（基于cv的Matcher）"></a>匹配（基于cv的Matcher）</h1><h2 id="汉明距离（Hamming-distance）"><a href="#汉明距离（Hamming-distance）" class="headerlink" title="汉明距离（Hamming distance）"></a>汉明距离（Hamming distance）</h2><p>用汉明距离（Hamming distance）作为两个二进制串之间的距离（差异），指不同位数的个数。这个在计算机硬件层面可以直接用异或实现，很友好。</p><p>在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。换句话说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数。例如：</p><p>​    1011101 与 1001001 之间的汉明距离是 2。<br>​    2143896 与 2233796 之间的汉明距离是 3。<br>​    “toned” 与 “roses” 之间的汉明距离是 3。</p><p>要决定有多少个位不同，只需将 xor 运算加诸于两个字码就可以，并在结果中计算有多个为1的位。两个字码中不同位值的数目称为汉明距离(Hamming distance) 。</p><h2 id="OpenCV中的Matcher"><a href="#OpenCV中的Matcher" class="headerlink" title="OpenCV中的Matcher"></a>OpenCV中的Matcher</h2><p>特征点匹配位于feature2D的模块中所以在使用的时候应该在头文件中加入：</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#include&lt;opencv2&#x2F;features2d&#x2F;features2d.hpp&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>模块中有三个类</p><p><img src=".%5Ctemp%5CSouthEast" class="lazyload placeholder" data-srcset=".%5Ctemp%5CSouthEast" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="这里写图片描述"></p><p>继承关系如下：</p><p><img src=".%5Ctemp%5Cmatcher%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB" class="lazyload placeholder" data-srcset=".%5Ctemp%5Cmatcher%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="这里写图片描述"></p><h3 id="cv-DescriptorMatcher"><a href="#cv-DescriptorMatcher" class="headerlink" title="cv::DescriptorMatcher"></a>cv::DescriptorMatcher</h3><p>DescriptorMatcher的类型：</p><p><img src=".%5Ctemp%5Cimage-20211230150851427.png" class="lazyload placeholder" data-srcset=".%5Ctemp%5Cimage-20211230150851427.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="image-20211230150851427"></p><p>可以<strong>直接使用 DescriptorMatcher</strong>，参数如上图👆，示例如下👇</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;BFMatcher 暴力匹配    vector&lt;DMatch&gt; BFmatches;    Ptr&lt;DescriptorMatcher&gt; BFMatcher &#x3D; DescriptorMatcher::create(&quot;BruteForce-Hamming&quot;);    &#x2F;&#x2F;Ptr&lt;BFMatcher&gt; Bmatcher &#x3D; BFMatcher::create();    BFMatcher-&gt;match(descriptors1, descriptors2, BFmatches);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="cv-BFMatcher"><a href="#cv-BFMatcher" class="headerlink" title="cv::BFMatcher"></a>cv::BFMatcher</h3><p><strong>Brute-Force匹配器</strong>很简单，它取第一个集合里的一个特征的描述子与第二个集合里所有其他特征和它通过一些距离计算进行匹配，返回匹配最近的值。</p><p>对于BF匹配器，OPENCV中首先得用 BFMatcher 类创建 BF 匹配器对象，它可取两个参数：第一个参数是 normType，它指定要使用的距离量度。默认是NORM_L2。对于SIFT,SURF很好。（还有NORM_L1）。对于二进制字符串的描述子，比如ORB，BRIEF，BRISK等，应该用NORM_HAMMING。使用Hamming距离度量，如果ORB使用VTA_K == 3或者4，应该用NORM_HAMMING2。第二个参数是布尔变量，crossCheck 默认值是false。如果设置为True，匹配条件就会更加严格，只有到A中的第i个特征点与B中的第j个特征点距离最近，并且B中的第j个特征点到A中的第i个特征点也是最近时才会返回最佳匹配(i,j)， 即这两个特征点要互相匹配才行。</p><p>BFMatcher对象有两个方法 BFMatcher.match() 和 BFMatcher.knnMatch()。第一个方法会返回最佳匹配。第二个方法为每个关键点返回k个最佳匹配，其中k是由用户设定的。</p><h3 id="cv-FlannBasedMatcher"><a href="#cv-FlannBasedMatcher" class="headerlink" title="cv::FlannBasedMatcher"></a>cv::FlannBasedMatcher</h3><p><strong>Flann-based matcher</strong> 使用快速近似最近邻搜索算法寻找（用快速的第三方库近似最近邻搜索算法），示例如下👇</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F;FlannBasedMatcher：    vector&lt;DMatch&gt; FlannMatches;    Ptr&lt;FlannBasedMatcher&gt; FlannMatcher &#x3D; FlannBasedMatcher::create();    FlannMatcher-&gt;match(descriptors1, descriptors2, FlannMatches);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>两种方法的比较</p><p>两者的区别在于BFMatcher总是尝试所有可能的匹配，从而使得它总能够找到最佳匹配，这也是Brute Force（暴力法）的原始含义。而FlannBasedMatcher中FLANN的含义是Fast Library for Approximate Nearest Neighbors，从字面意思可知它是一种近似法，算法更快但是找到的是最近邻近似匹配，所以当我们需要找到一个相对好的匹配但是不需要最佳匹配的时候往往使用FlannBasedMatcher。当然也可以通过调整FlannBasedMatcher的参数来提高匹配的精度或者提高算法速度，但是相应地算法速度或者算法精度会受到影响。</p><p>此外，使用特征提取过程得到的特征描述符（descriptor）数据类型有的是float类型的，比如SurfDescriptorExtractor，SiftDescriptorExtractor。有的是uchar类型的，比如说有ORB，BriefDescriptorExtractor。对应uchar类型的匹配方式只有BruteForce（大概？）。所以ORB和BRIEF特征描述子只能使用BruteForce匹配法。</p><h2 id="匹配优化"><a href="#匹配优化" class="headerlink" title="匹配优化"></a>匹配优化</h2><p>在比对描述值相似度的方法中，最简单直观的方法就是使用暴力匹配方法(Brute-Froce Matcher)，即计算某一个特征点描述子与其他所有特征点描述子之间的距离，然后将得到的距离进行排序，取距离最近的一个作为匹配点。这种方法简单粗暴，其结果也是显而易见的，但也可能有大量的错误匹配，这就需要使用一些机制来过滤掉错误的匹配。</p><h3 id="1-汉明距离小于最小距离的两倍"><a href="#1-汉明距离小于最小距离的两倍" class="headerlink" title="1. 汉明距离小于最小距离的两倍"></a>1. 汉明距离小于最小距离的两倍</h3><p>经典的方法有汉明距离小于最小距离的两倍，选择已经匹配的点对的汉明距离不大于最小距离的两倍作为判断依据，如果不大于该值则认为是一个正确的匹配；大于该值则认为是一个错误的匹配。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">&#x2F;&#x2F; 匹配对筛选double min_dist &#x3D; 1000, max_dist &#x3D; 0;&#x2F;&#x2F; 找出所有匹配之间的最大值和最小值for (int i &#x3D; 0; i &lt; descriptors1.rows; i++)&#123;    double dist &#x3D; matches[i].distance;    if (dist &lt; min_dist) min_dist &#x3D; dist;    if (dist &gt; max_dist) max_dist &#x3D; dist;&#125;&#x2F;&#x2F; 当描述子之间的匹配大于2倍的最小距离时，即认为该匹配是一个错误的匹配。&#x2F;&#x2F; 但有时描述子之间的最小距离非常小，可以设置一个经验值作为下限vector&lt;DMatch&gt; good_matches;for (int i &#x3D; 0; i &lt; descriptors1.rows; i++)&#123;    if (matches[i].distance &lt;&#x3D; max(2 * min_dist, 30.0))        good_matches.push_back(matches[i]);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-交叉匹配（BFMatcher的第二个参数可以实现）"><a href="#2-交叉匹配（BFMatcher的第二个参数可以实现）" class="headerlink" title="2. 交叉匹配（BFMatcher的第二个参数可以实现）"></a>2. 交叉匹配（BFMatcher的第二个参数可以实现）</h3><p>针对暴力匹配，可以使用交叉匹配的方法来过滤错误的匹配。交叉过滤的思想是再进行一次匹配，反过来使用被匹配到的点进行匹配，如果匹配到的仍然是第一次匹配的点的话，就认为这是一个正确的匹配。举例来说就是，假如第一次特征点A使用暴力匹配的方法，匹配到的特征点是特征点B；反过来，使用特征点B进行匹配，如果匹配到的仍然是特征点A，则就认为这是一个正确的匹配，否则就是一个错误的匹配。OpenCV中BFMatcher已经封装了该方法，创建BFMatcher的实例时，第二个参数传入true即可，BFMatcher bfMatcher(NORM_HAMMING,true)。</p><h3 id="3-KNN匹配"><a href="#3-KNN匹配" class="headerlink" title="3. KNN匹配"></a>3. KNN匹配</h3><p>在匹配过程中，为了排除因为图像遮挡和背景混乱而产生的无匹配关系的关键点，SIFT的作者Lowe提出了比较最近邻距离与次近邻距离的SIFT匹配方式：取一幅图像中的一个SIFT关键点，并找出其与另一幅图像中欧式距离最近的前两个关键点，在这两个关键点中，如果最近的距离除以次近的距离得到的比率ratio少于某个阈值T，则接受这一对匹配点。因为对于错误匹配，由于特征空间的高维性，相似的距离可能有大量其他的错误匹配，从而它的ratio值比较高。显然降低这个比例阈值T，SIFT匹配点数目会减少，但更加稳定，反之亦然。</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const float minRatio &#x3D; 1.f &#x2F; 1.5f;const int k &#x3D; 2; vector&lt;vector&lt;DMatch&gt;&gt; knnMatches;matcher-&gt;knnMatch(leftPattern-&gt;descriptors, rightPattern-&gt;descriptors, knnMatches, k); for (size_t i &#x3D; 0; i &lt; knnMatches.size(); i++) &#123;    const DMatch&amp; bestMatch &#x3D; knnMatches[i][0];    const DMatch&amp; betterMatch &#x3D; knnMatches[i][1];     float  distanceRatio &#x3D; bestMatch.distance &#x2F; betterMatch.distance;    if (distanceRatio &lt; minRatio)        matches.push_back(bestMatch);&#125;const  float minRatio &#x3D;  1.f  &#x2F;  1.5f;const  int k &#x3D;  2; vector&lt;vector&lt;DMatch&gt;&gt; knnMatches;matcher-&gt;knnMatch(leftPattern-&gt;descriptors, rightPattern-&gt;descriptors, knnMatches, 2); for (size_t i &#x3D;  0; i &lt; knnMatches.size(); i++) &#123;    const DMatch&amp; bestMatch &#x3D; knnMatches[i][0];    const DMatch&amp; betterMatch &#x3D; knnMatches[i][1];    float distanceRatio &#x3D; bestMatch.distance  &#x2F; betterMatch.distance;    if (distanceRatio &lt; minRatio)        matches.push_back(bestMatch);&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-通过RANSAC寻找单应性矩阵的方法"><a href="#4-通过RANSAC寻找单应性矩阵的方法" class="headerlink" title="4. 通过RANSAC寻找单应性矩阵的方法"></a>4. 通过RANSAC寻找单应性矩阵的方法</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/12/31/ICP/"/>
      <url>/2021/12/31/ICP/</url>
      
        <content type="html"><![CDATA[<h1 id="3d-3d问题"><a href="#3d-3d问题" class="headerlink" title="3d-3d问题"></a>3d-3d问题</h1><h2 id="ICP"><a href="#ICP" class="headerlink" title="ICP"></a>ICP</h2><p>Iterative Closest Point<br>一个简单的最小二乘问题<br>信息量是关键，可以见到从2d-2d的八点到3d-2d的p3p，逐渐从能否求解的问题变成优化问题</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/12/31/3D-3D/"/>
      <url>/2021/12/31/3D-3D/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/12/31/3D-2D/"/>
      <url>/2021/12/31/3D-2D/</url>
      
        <content type="html"><![CDATA[<p>引出：</p><p>2D-2D的对极几何需要8个以上点对，且存在初始化、纯旋转和尺度问题。</p><p>特征点3D位置可以由三角化或RGB-D相机深度图确定。</p><p>因此，双目或RGB-D相机直接使用PnP估计相机运动。单目视觉里程计，首先初始化，然后才能PnP。</p><p>PnP为（ Perspective-n-Point）的简称，即给出n个3D空间点及其投影位置时，如何求解相机的位姿R t。</p><p>PnP优点：不需要对极约束，3个的匹配点对就可以运动估计。</p><p>内容：</p><p>直接线性变换（DLT）</p><p>P3P及实现</p><p>BA（Bundle Adjustment）及实现</p><p>一、DLT</p><p>每个特征点对提供两个关于t的线性约束，t 一共12维，至少需要6个特征点对实现T的线性求解。<br>匹配点数大于6，SVD最小二乘解。<br>缺点：T矩阵看成12个未知数，忽略了他们之间的联系。</p><p>二、P3P<br>1、原理：利用了三角形相似，求解投影点abc在相机坐标系下的3D坐标，然后将问题转为3D-3D问题。</p><p>在SLAM实用：先P3P估计相机位姿R t， 之后构建最小二乘优化问题对估计值进行调整。</p><p>2、代码实现<br>调用solvePnP函数<br>bool cv::solvePnP(objectPoints,imagePoints,cameraMatrix,distCoeffs,<br>OutputArray r, OutputArray t,<br>bool useExtrinsicGuess = false,int flags = SOLVEPNP_ITERATIVE )<br>objectPoints    Array of object points in the 世界坐标系, Nx3 1-channel or 1xN/Nx1 3-channel, where N is the number of points. vector<Point3f>  pts_3d<br>imagePoints    Array of corresponding 第二张图的像素坐标, Nx2 1-channel or 1xN/Nx1 2-channel, where N is the number of points. vector<Point2f> pts_2d<br>cameraMatrix    Input camera matrix K<br>distCoeffs    Input vector of distortion coefficients (k1,k2,p1,p2[,k3[,k4,k5,k6[,s1,s2,s3,s4[,τx,τy]]]]) of 4, 5, 8, 12 or 14 elements. If the vector is NULL/empty, the zero distortion coefficients are assumed.<br>rvec    Output 旋转向量 (see Rodrigues ) that, together with tvec , brings points from the model coordinate system to the camera coordinate system.需要罗德里格斯公式化为旋转矩阵<br>tvec    Output translation vector.<br>useExtrinsicGuess    Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the provided rvec and tvec values as initial approximations of the rotation and translation vectors, respectively, and further optimizes them<br>缺点：P3P只利用了3个点信息<br>如果3D或2D点受到噪声影响，或者误匹配，算法失效。<br> 主框架</p><p>#include <iostream><br>#include <vector><br>#include &lt;opencv2/opencv.hpp&gt;<br>using namespace std;<br>using namespace cv;</p><p>void find_feature_matches(<br>    const Mat&amp; img_1, const Mat&amp; img_2,<br>    std::vector<KeyPoint>&amp; keypoints_1,<br>    std::vector<KeyPoint>&amp; keypoints_2,<br>    std::vector&lt; DMatch &gt;&amp; matches);</p><p>Point2d pixel2cam(const Point2d&amp; p, const Mat&amp; K);</p><p>void bundleAdjustment(<br>    const vector<Point3f> points_3d,<br>    const vector<Point2f> points_2d,<br>    const Mat&amp; K,<br>    Mat&amp; R, Mat&amp; t<br>    );<br>int main()<br>{<br>    Mat img1 = imread(“1.png”);<br>    Mat img2 = imread(“2.png”);<br>    Mat d1 = imread(“1_depth.png”);       // 深度图为16位无符号数，单通道图像<br>    //1. 找到特征点和匹配关系<br>    vector<KeyPoint> keypoint1, keypoint2;<br>    vector<DMatch> matches;<br>    find_feature_matches(img1,img2,keypoint1,keypoint2,matches);</p><pre><code>Mat K = (Mat_&lt;double&gt;(3, 3) &lt;&lt; 520.9, 0, 325.1, 0, 521.0, 249.7, 0, 0, 1);//2. 第一张图像给出3D坐标，第二张给像素2d坐标vector&lt;Point3f&gt; pts_3d;vector&lt;Point2f&gt; pts_2d;for (DMatch m : matches)&#123;     //3. 3D坐标由第一张的像素坐标转为相机坐标，再乘上深度，为世界坐标    ushort d = d1.ptr&lt;unsigned short&gt; (int(keypoint1[m.queryIdx].pt.y))[int (keypoint1[m.queryIdx].pt.x)];    d = d / 1000;    Point2d p1 = pixel2cam(keypoint1[m.queryIdx].pt, K);    Point3f p2(p1.x*d, p1.y*d, d);    pts_3d.push_back(p2);    pts_2d.push_back(keypoint2[m.trainIdx].pt);    cout &lt;&lt; &quot;3d-2d pairs&quot; &lt;&lt; pts_3d.size() &lt;&lt; endl;    //4. 调用函数p3p，求解r，t    Mat R,r, t;    solvePnP(pts_3d, pts_2d, K, Mat(), r, t, false, SOLVEPNP_EPNP);    Rodrigues(r, R); &#125;</code></pre><p>}<br> 注解：<br>Mat d1 = imread(“1_depth.png”);<br>ushort d = d1.ptr<unsigned short> (int(keypoint1[m.queryIdx].pt.y))[int(keypoint1[m.queryIdx].pt.x)];<br>d1是深度图，Mat类型的ptr是指针，获得像素点深度，首先得到行地址，之后列指针  (pt.y)[pt.x]</p><p>第一张图像给出3D坐标，第二张给像素2d坐标; 其中3D坐标由第一张的像素坐标转为相机坐标，再乘上深度，为世界坐标。</p><p>Rodrigues(r, R);<br>solvepnp得到的是旋转向量r，需要通过罗德里格斯公式换为旋转矩阵R。即李代数向李群的转换。</p><p>三、BA<br>这种优化方法用在SLAM上时称为集束调整（Bundle Adjustment，BA）。</p><p>“集束调整”名称的含义是说，通过调整相机的姿态使3D路标点发出的光线都能汇聚到相机的光心。</p><p>回顾g2o图优化库，优化变量作为顶点，误差项作为变，构造一个图。</p><p>本节，顶点是3D坐标(XYZ)和相机位姿R t,误差项是重投影误差。把问题建模成一个最小二乘的图优化问题。</p><p>void bundleAdjustment (<br>    const vector&lt; Point3f &gt; points_3d,<br>    const vector&lt; Point2f &gt; points_2d,<br>    const Mat&amp; K,<br>    Mat&amp; R, Mat&amp; t )<br>{<br>    // 1. 初始化g2o，设定线性方程求解器Blocksolver和迭代算法<br>    typedef g2o::BlockSolver&lt; g2o::BlockSolverTraits&lt;6,3&gt; &gt; Block;  // pose维度为 6, 观测landmark维度为 3<br>    Block::LinearSolverType* linearSolver = new g2o::LinearSolverCSparse<a href="Block::PoseMatrixType">Block::PoseMatrixType</a>();<br>    Block* solver_ptr = new Block ( linearSolver ); </p><pre><code>g2o::OptimizationAlgorithmLevenberg* solver = new g2o::OptimizationAlgorithmLevenberg ( solver_ptr );//图优化，设置求解器g2o::SparseOptimizer optimizer;optimizer.setAlgorithm ( solver ); // 2.1 图中增加顶点位姿poseg2o::VertexSE3Expmap* pose = new g2o::VertexSE3Expmap(); // 相机的李代数位姿，VertexSE3Expmap类Eigen::Matrix3d R_mat;R_mat &lt;&lt;      R.at&lt;double&gt; ( 0,0 ), R.at&lt;double&gt; ( 0,1 ), R.at&lt;double&gt; ( 0,2 ),           R.at&lt;double&gt; ( 1,0 ), R.at&lt;double&gt; ( 1,1 ), R.at&lt;double&gt; ( 1,2 ),           R.at&lt;double&gt; ( 2,0 ), R.at&lt;double&gt; ( 2,1 ), R.at&lt;double&gt; ( 2,2 );pose-&gt;setId ( 0 );//相机位姿类型SE3Quat,四元数+位移向量pose-&gt;setEstimate ( g2o::SE3Quat (                        R_mat,                        Eigen::Vector3d ( t.at&lt;double&gt; ( 0,0 ), t.at&lt;double&gt; ( 1,0 ), t.at&lt;double&gt; ( 2,0 ) )                    ) );optimizer.addVertex ( pose );// 2.2 图中增加顶点坐标pointint index = 1;for ( const Point3f p:points_3d ) &#123;        g2o::VertexSBAPointXYZ* point = new g2o::VertexSBAPointXYZ();//空间点位置类型，VertexSBAPointXYZ    point-&gt;setId ( index++ );    point-&gt;setEstimate ( Eigen::Vector3d ( p.x, p.y, p.z ) );    point-&gt;setMarginalized ( true ); // g2o 中必须设置 marg 参见第十讲内容    optimizer.addVertex ( point );&#125; // 相机内参K，CameraParameters类g2o::CameraParameters* camera = new g2o::CameraParameters (    K.at&lt;double&gt; ( 0,0 ), Eigen::Vector2d ( K.at&lt;double&gt; ( 0,2 ), K.at&lt;double&gt; ( 1,2 ) ), 0);camera-&gt;setId ( 0 );optimizer.addParameter ( camera ); // 3. 图中增加边index = 1;for ( const Point2f p:points_2d )&#123;    g2o::EdgeProjectXYZ2UV* edge = new g2o::EdgeProjectXYZ2UV();//投影方程边类型EdgeProjectXYZ2UV    edge-&gt;setId ( index );    edge-&gt;setVertex ( 0, dynamic_cast&lt;g2o::VertexSBAPointXYZ*&gt; ( optimizer.vertex ( index ) ) );    edge-&gt;setVertex ( 1, pose );    edge-&gt;setMeasurement ( Eigen::Vector2d ( p.x, p.y ) );    edge-&gt;setParameterId ( 0,0 );    edge-&gt;setInformation ( Eigen::Matrix2d::Identity() );    optimizer.addEdge ( edge );    index++;&#125; //4. 执行优化optimizer.initializeOptimization();optimizer.optimize ( 100 ); cout&lt;&lt;endl&lt;&lt;&quot;after optimization:&quot;&lt;&lt;endl;cout&lt;&lt;&quot;T=&quot;&lt;&lt;endl&lt;&lt;Eigen::Isometry3d ( pose-&gt;estimate() ).matrix() &lt;&lt;endl;</code></pre><p>}<br> VertexSE3Expmap</p><p>相机位姿顶点类VertexSE3Expmap<br>3D路标点类VertexSBAPointXYZ<br>重投影误差边类EdgeProjectXYZ2UV<br>//继承了 BaseVertex &lt;6, SE3Quat&gt; 6维李代数，相机位姿SE3Quat李代数<br>class G2O_TYPES_SBA_API VertexSE3Expmap : public BaseVertex&lt;6, SE3Quat&gt;{<br>public:<br>  EIGEN_MAKE_ALIGNED_OPERATOR_NEW<br>  //1. 默认构造函数<br>  VertexSE3Expmap();<br>  //2. 重置函数<br>  virtual void setToOriginImpl() {<br>    <em>estimate = SE3Quat();<br>  }<br>  //3. 更新函数<br>  virtual void oplusImpl(const double* update</em>)  {<br>    //成员函数Map映射成6维向量<br>    Eigen::Map<const Vector6d> update(update_);<br>    //李代数上增量左乘<br>    setEstimate(SE3Quat::exp(update)*estimate());<br>  }<br>  //4. 存盘和读盘：留空<br>  bool read(std::istream&amp; is);<br>  bool write(std::ostream&amp; os) const;<br>};<br>EdgeProjectXYZ2UV::computeError()</p><p>//继承了BaseBinaryEdge类，观测值是2维，类型Vector2D,<br>class G2O_TYPES_SBA_API EdgeProjectXYZ2UV : public  BaseBinaryEdge&lt;2, Vector2D, VertexSBAPointXYZ, VertexSE3Expmap&gt;{<br>  public:<br>    EIGEN_MAKE_ALIGNED_OPERATOR_NEW;<br>    //1. 默认初始化<br>    EdgeProjectXYZ2UV();<br>    //2. 计算误差<br>    void computeError()  {<br>      //李代数相机位姿v1<br>      const VertexSE3Expmap* v1 = static_cast&lt;const VertexSE3Expmap*&gt;(_vertices[1]);<br>      // 顶点v2<br>      const VertexSBAPointXYZ* v2 = static_cast&lt;const VertexSBAPointXYZ*&gt;(_vertices[0]);<br>      //相机参数<br>      const CameraParameters * cam<br>        = static_cast&lt;const CameraParameters <em>&gt;(parameter(0));<br>     //误差计算，测量值减去估计值，也就是重投影误差obs-cam<br>     //估计值计算方法是T</em>p,得到相机坐标系下坐标，然后在利用camera2pixel()函数得到像素坐标。<br>      Vector2D obs(_measurement);<br>      _error = obs-cam-&gt;cam_map(v1-&gt;estimate().map(v2-&gt;estimate()));<br>    }<br>    //3. 线性增量函数，也就是雅克比矩阵J的计算方法<br>    virtual void linearizeOplus();<br>    //4. 相机参数<br>    CameraParameters * _cam;</p><pre><code>bool read(std::istream&amp; is);bool write(std::ostream&amp; os) const;</code></pre><p>};<br>.map映射<br><em>error = <em>measurement - camera</em>-&gt;camera2pixel(pose-&gt;estimate().map(point</em>) );<br>里面有个map()函数。看一下这个函数的源码：</p><pre><code>  Vector3D map(const Vector3D &amp; xyz) const  &#123;    return _r*xyz + _t;  &#125;</code></pre><p> 传入一个3D点，返回r*p+t，很明显就是求变换后点的坐标。<br>在g2o中，用SE3Quat类型表示变换T，此类型中有个成员函数就是map()，作用为对一个3D点进行坐标变换，</p><p>EdgeProjectXYZ2UV::linearizeOplus()</p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void EdgeProjectXYZ2UVPoseOnly::linearizeOplus()&#123;    &#x2F;**     * 这里说一下整体思路：     * 重投影误差的雅克比矩阵在书中P164页式7.45已经呈现，所以这里就是直接构造，     * 构造时发现需要变换后的空间点坐标，所以需要先求出。     *&#x2F;    &#x2F;&#x2F;1. 首先还是从顶点取出位姿    g2o::VertexSE3Expmap* pose &#x3D; static_cast&lt;g2o::VertexSE3Expmap*&gt; ( _vertices[1] );    &#x2F;&#x2F;2. 这由位姿构造一个四元数形式T    g2o::SE3Quat T ( pose-&gt;estimate() );    &#x2F;&#x2F;3. 用T求得变换后的3D点坐标。T.map（p）就是T*p    g2o::VertexSBAPointXYZ* point_ &#x3D; static_cast&lt;g2o::VertexSBAPointXYZ*&gt;(_vertices[0]);    Vector3d xyz_trans &#x3D; T.map ( point_ );    &#x2F;&#x2F;OK，到这步，变换后的3D点xyz坐标就分别求出来了，后面的z平方，纯粹是为了后面构造J时方便定义的，因为需要多处用到    double x &#x3D; xyz_trans[0];    double y &#x3D; xyz_trans[1];    double z &#x3D; xyz_trans[2];    double z_2 &#x3D; z*z;    &#x2F;&#x2F;4. 相机参数    const CameraParameters* cam&#x3D;static_cast&lt;const CameraParameters*&gt;(parameter(0));&#x2F;&#x2F;4. 直接各个元素构造J就好了，对照式7.45是一模一样的，2*6的矩阵。_jacobianOplusXi ( 0,0 ) &#x3D;  x*y&#x2F;z_2 *camera_-&gt;fx_;_jacobianOplusXi ( 0,1 ) &#x3D; - ( 1+ ( x*x&#x2F;z_2 ) ) *camera_-&gt;fx_;_jacobianOplusXi ( 0,2 ) &#x3D; y&#x2F;z * camera_-&gt;fx_;_jacobianOplusXi ( 0,3 ) &#x3D; -1.&#x2F;z * camera_-&gt;fx_;_jacobianOplusXi ( 0,4 ) &#x3D; 0;_jacobianOplusXi ( 0,5 ) &#x3D; x&#x2F;z_2 * camera_-&gt;fx_; _jacobianOplusXi ( 1,0 ) &#x3D; ( 1+y*y&#x2F;z_2 ) *camera_-&gt;fy_;_jacobianOplusXi ( 1,1 ) &#x3D; -x*y&#x2F;z_2 *camera_-&gt;fy_;_jacobianOplusXi ( 1,2 ) &#x3D; -x&#x2F;z *camera_-&gt;fy_;_jacobianOplusXi ( 1,3 ) &#x3D; 0;_jacobianOplusXi ( 1,4 ) &#x3D; -1.&#x2F;z *camera_-&gt;fy_;_jacobianOplusXi ( 1,5 ) &#x3D; y&#x2F;z_2 *camera_-&gt;fy_;&#125;&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>注意：</p><p>1、相机位姿顶点类VertexSE3Expmap使用了李代数表示相机位姿，而不是使用旋转矩阵和平移矩阵。</p><p>这是因为旋转矩阵是有约束的矩阵，它必须是正交矩阵且行列式为1。使用它作为优化变量就会引入额外的约束条件，从而增大优化的复杂度。而将旋转矩阵通过李群-李代数之间的转换关系转换为李代数表示，就可以把位姿估计变成无约束的优化问题，求解难度降低。</p><p>2、在重投影误差边类EdgeProjectXYZ2UV中，已经为相机位姿和3D点坐标推导了雅克比矩阵，以计算迭代的增量方向。<br>————————————————<br>版权声明：本文为CSDN博主「try_again_later」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/try_again_later/article/details/81813639">https://blog.csdn.net/try_again_later/article/details/81813639</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/12/31/%E8%A7%86%E8%A7%89%E9%87%8C%E7%A8%8B%E8%AE%A1%E6%A6%82%E8%BF%B0/"/>
      <url>/2021/12/31/%E8%A7%86%E8%A7%89%E9%87%8C%E7%A8%8B%E8%AE%A1%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="视觉里程计"><a href="#视觉里程计" class="headerlink" title="视觉里程计"></a>视觉里程计</h1><p>​        一个SLAM系统分为前端和后端，其中前端又称为视觉里程计，作用为根据相邻图像信息推断相机运动，给后端一个较好的初值。</p><h2 id="视觉里程计分类"><a href="#视觉里程计分类" class="headerlink" title="视觉里程计分类"></a>视觉里程计分类</h2><p>​        主要分为两个大类，特征点法与直接法。</p><h2 id="特征点法"><a href="#特征点法" class="headerlink" title="特征点法"></a>特征点法</h2><p>​        角点提取算法有很多，例如<strong>Harris、FAST、GFTT</strong>等（上古算法）。计算机视觉领域常用特征点：<strong>SIFT、SURF、ORB</strong>等。这些人工设计的特征点有更好的特质：可重复性、可区别性、高效率、本地性。</p><p>​        特征点由两部分组成：关键点（Key-point）、描述子（Descriptor）。</p><p>​        <strong>SIFT</strong>（尺度不变特征变换，Scale-Invariant Feature Transform）<br>​            –充分考虑各种因素，但计算量大。<br>​        <strong>ORB</strong>（Oriented FAST and Rotated BRIEF）<br>​            –改进的FAST，速度快，很无敌。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/12/31/%E4%B8%89%E8%A7%92%E6%B5%8B%E9%87%8F/"/>
      <url>/2021/12/31/%E4%B8%89%E8%A7%92%E6%B5%8B%E9%87%8F/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2021/12/31/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%952d-2d/"/>
      <url>/2021/12/31/%E5%AF%B9%E6%9E%81%E5%87%A0%E4%BD%952d-2d/</url>
      
        <content type="html"><![CDATA[<h1 id="2D-2D：对极几何"><a href="#2D-2D：对极几何" class="headerlink" title="2D-2D：对极几何"></a>2D-2D：对极几何</h1><h2 id="个人理解"><a href="#个人理解" class="headerlink" title="个人理解"></a>个人理解</h2><p>对极几何是<strong>双视图</strong>之间内在的射影几何。它独立于景物结构，只依赖于摄像机的内参外参。</p><p>适用范围：有前后两帧相机图像，但是没有深度信息</p><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p><img src=".%5Ctemp%5Cepipolar1.png" class="lazyload placeholder" data-srcset=".%5Ctemp%5Cepipolar1.png" srcset="https://img2.baidu.com/it/u=2037979560,2772131037&fm=26&fmt=auto&gp=0.jpg" alt="img"></p><p>上图a中两个摄像机的中心分别是C与C’，而X为一个三维空间点，它在两个摄像机的成像平面上的投影点分别是x与x’。我们常称：</p><ul><li><strong>基线</strong>：两个摄像机光心的连线CC′CC′</li><li><strong>对极点</strong>：上图b中的e与e’，它们分别是一幅视图中另一个摄像机中心的像。二维表示为基线CC′CC′与两个成像平面的交点。</li><li><strong>对极平面</strong>：是一张包含基线的平面，存在着对极平面的一个集合（以基线为轴转动），上图中的一个例子就是CXC′CXC′。</li><li><strong>对极线</strong>：对极平面与图像平面的交线。上图中的例子是$xe$与$x’e’$，一个成像平面上的所有的对极线相交于对极点。</li></ul><blockquote><p>上图中还体现了一些性质，对于一幅图像上的每一点x，在第二幅图像上，任何与该点x匹配的点x’必然在对极线I’上，该对极线是过点x与第一个摄像机中心C的射线在第二幅图像上的投影。</p></blockquote><h2 id="对极约束"><a href="#对极约束" class="headerlink" title="对极约束"></a>对极约束</h2><h3 id="一个公式总结"><a href="#一个公式总结" class="headerlink" title="一个公式总结"></a>一个公式总结</h3><p>$$<br>x_{2}^T t^{\land} R x_1 = 0（x1,x2是归一化平面上的坐标）<br>$$</p><h3 id="对极约束的推导"><a href="#对极约束的推导" class="headerlink" title="对极约束的推导"></a>对极约束的推导</h3><p>从相机模型的学习中我们可以理解如下的公式(假定第一帧即世界坐标系)：<br>$$<br>Z_1p_1 = KP_w<br>$$</p><p>$$<br>Z_2p_2 = K(RP_w + t)<br>$$<br>其中K是相机内参，R与t是第二个相机在第一个相机的相机坐标系下的外参，$P_w$是此空间点在第一个相机的相机坐标系下的坐标。Z是空间点到相机光心的距离(也是相机坐标系下的z轴坐标)。$p_1$与$p_2$是空间点$P_w$在两个相机平面上的投影点。</p><ul><li><p>首先做出如下定义（其中x1与x2是归一化的相机坐标（X/Z,Y/Z,1））：<br>$$<br>x_1 = K^{-1}p_1\quad,\quad x_2 = K^{-1}p_2<br>$$</p></li><li><p>带入如上定义可得：<br>$$<br>Z_2 x_2 = Z_1 Rx_1 + t<br>$$</p></li><li><p>上式两边同时左乘<code>t^</code>，也就是两侧同时与t做外积：<br>$$<br>Z_2 t^{\land} x_2 = Z_1 t^{\land} R x_1<br>$$</p></li><li><p>两侧同时左乘$x^T_2$：<br>$$<br>Z_2 x_{2}^T t^{\land} x_2 = Z_1 x_{2}^T t^{\land} R x_1<br>$$</p></li><li><p>上式中由于等式左侧$t^{\land} x_2$是一个与t、x2都垂直的向量，再和x2做内积将得到0，因此上式子其实可以写成如下形式：<br>$$<br>x_{2}^T t^{\land} R x_1 = 0<br>$$</p></li><li><p>上式就是大名鼎鼎的<strong>对极约束</strong>，将 x1 与 x2 换下，重新带入 p1 与 p2 可得：<br>$$<br>p_2^T K^{-T} t^{\land} R K^{-1} p_1 = 0<br>$$<br>由对极约束的两种形态可以导出两种矩阵：</p></li><li><p><strong>本质矩阵</strong>是基础矩阵中只与外参相关的部分，即去掉相机内参，可以得到：<br>$$<br>E = t^{\land} R<br>$$</p></li></ul><p>$$<br>x_{2}^T E R x_1 = 0<br>$$</p><ul><li><strong>基础矩阵</strong>，如下：<br>$$<br>F = K_2^{-T} E K_1^{-1}<br>$$</li></ul><p>$$<br>p_{uv2}^T F p_{uv1} = 0<br>$$</p><blockquote><p>此处注意，当点p1与基础矩阵F确定时，上式演变成一个直线方程，更好的诠释了基础矩阵的本质是<strong>从一个点到一条直线的射影映射的代数表示。</strong></p></blockquote><h2 id="本质矩阵求解"><a href="#本质矩阵求解" class="headerlink" title="本质矩阵求解"></a>本质矩阵求解</h2><h3 id="代数方法"><a href="#代数方法" class="headerlink" title="代数方法"></a>代数方法</h3><p>理论上6-1自由度只需要5点即可求解，但是实现比较困难，更多用E矩阵的线性性质，也就是<strong>八点法</strong>（Eight-point-algorithm）</p><p>将本质矩阵形式的对极约束方程 $x_{2}^T E R x_1 = 0$ 写成线性形式<br>$$<br>\left(\begin{array}{ccccccccc}<br>u_{2}^{1} u_{1}^{1} &amp; u_{2}^{1} v_{1}^{1} &amp; u_{2}^{1} &amp; v_{2}^{1} u_{1}^{1} &amp; v_{2}^{1} v_{1}^{1} &amp; v_{2}^{1} &amp; u_{1}^{1} &amp; v_{1}^{1} &amp; 1 \<br>u_{2}^{2} u_{1}^{2} &amp; u_{2}^{2} v_{1}^{2} &amp; u_{2}^{2} &amp; v_{2}^{2} u_{1}^{2} &amp; v_{2}^{2} v_{1}^{2} &amp; v_{2}^{2} &amp; u_{1}^{2} &amp; v_{1}^{2} &amp; 1 \<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \<br>u_{2}^{8} u_{1}^{8} &amp; u_{2}^{8} v_{1}^{8} &amp; u_{2}^{8} &amp; v_{2}^{8} u_{1}^{8} &amp; v_{2}^{8} v_{1}^{8} &amp; v_{2}^{8} &amp; u_{1}^{8} &amp; v_{1}^{8} &amp; 1<br>\end{array}\right)\left(\begin{array}{c}<br>e_{1} \<br>e_{2} \<br>e_{3} \<br>e_{4} \<br>e_{5} \<br>e_{6} \<br>e_{7} \<br>e_{8} \<br>e_{9}<br>\end{array}\right)=0<br>$$<br>系数矩阵满秩时，零空间维度为1，这与e的<strong>尺度等价性</strong>是一致的。</p><p>从E恢复到R，t，可以使用SVD分解得到如下等式👇<br>$$<br>\boldsymbol{E}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}}<br>$$</p><p>其中U和V是正交阵，中间Σ是奇异值矩阵。根据E的内在性质，我们知道$Σ = diag(μ,μ,0)$。通过SVD分解，任意一个E，存在两个可能的R和t与其对应。<br>$$<br>t_{1}^{\wedge}=U R_{Z}\left(\frac{\pi}{2}\right) \Sigma U^{\mathrm{T}}, \quad<br>R_{1}=U R_{Z}^{\mathrm{T}}\left(\frac{\pi}{2}\right) V^{\mathrm{T}}  \<br>t_{2}^{\wedge}=U R_{Z}\left(-\frac{\pi}{2}\right) \Sigma U^{\mathrm{T}}, \quad<br>R_{2}=U R_{Z}^{\mathrm{T}}\left(-\frac{\pi}{2}\right) V^{\mathrm{T}}<br>$$<br>其中$R_{Z}\left(\frac{\pi}{2}\right)$表示沿Z轴旋转90°得到的旋转矩阵。同时由于E无论是否取符号都是等价的，因此对上面得到的t取负号也会得到相同的结果。因此每个E通过分解一共有四种可能的解。</p><h3 id="超定方法（增加鲁棒性）"><a href="#超定方法（增加鲁棒性）" class="headerlink" title="超定方法（增加鲁棒性）"></a>超定方法（增加鲁棒性）</h3><ol><li>构建最小二乘</li><li>RANSAC（Random Sample Consensus）随机采样一致性求解，这是更通用的方法</li></ol><h2 id="单应矩阵"><a href="#单应矩阵" class="headerlink" title="单应矩阵"></a>单应矩阵</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/12/31/hello-world/"/>
      <url>/2021/12/31/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
